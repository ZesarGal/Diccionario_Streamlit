import numpy as np
import matplotlib.pyplot as plt
import cvxopt as opt
from cvxopt import blas, solvers
import pandas as pd
import pylab
from PIL import Image
import streamlit as st
import streamlit as st
import numpy as np
import plotly.graph_objects as go

from activations import derivative, logistic, relu, elu, selu_alpha, selu_scale, selu, plot_function, plot_function_derivative


st.markdown("# C√°lculo üëΩ")
st.sidebar.markdown("# C√°lculo üëΩ")

activation_function = st.selectbox('Definiciones', ['None', 'Funci√≥n log√≠stica (sigmoidea)', 'Funci√≥n tangente hiperb√≥lica (Tanh)', 'Derivada Parcial', 'Multiplicadores de Lagrange', 'Matriz Jacobiana', 'Funci√≥n unitaria lineal exponencial','Bibliografia Sugerida'])

## Funci√≥n Log√≠stica
if activation_function == 'Funci√≥n log√≠stica (sigmoidea)':

    st.header('Funci√≥n log√≠stica (sigmoidea)')

    st.subheader('Descripci√≥n')
    st.write('Es una funci√≥n sigmoidea con una curva caracter√≠stica en forma de "S".')
    st.markdown(r'**$sigmoid(z)=\frac{1}{1+exp(-z)}$**')
    st.write('La salida de la funci√≥n log√≠stica (sigmoide) est√° siempre entre 0 y 1.')   

    st.subheader('Plot')
    logistic_fig  = plot_function(logistic, title='Logistic (Sigmoid) Activation Function')
    logistic_fig.add_annotation(x=7, y=1, text='<b>Saturation</b>', showarrow=True,
     font=dict(family="Montserrat", size=16, color="#1F8123"),
        align="center",arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor="#A835E1", ax=-20, ay=30,)
    logistic_fig.add_annotation(x=-7, y=0, text='<b>Saturation</b>', showarrow=True,
     font=dict(family="Montserrat", size=16, color="#1F8123"),
        align="center",arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor="#A835E1", ax=0, ay=-30,)
    st.plotly_chart(logistic_fig)
    with st.expander('Plot Explanation'):
        st.write('- La funci√≥n log√≠stica se satura a medida que las entradas son mayores (positivas o negativas).')
        st.write('- Para valores positivos y negativos grandes, la funci√≥n se acerca asint√≥ticamente a 1 y 0, respectivamente.')
        st.write('- Cuando la funci√≥n se satura, su gradiente se aproxima mucho a cero, lo que ralentiza el aprendizaje..')

    st.subheader('Derivada')
    st.markdown(r'$sigmoid^{\prime}(z)=sigmoid(z)(1‚àísigmoid(z))$')
    st.text("")
    logistic_der_fig = plot_function_derivative(logistic, title='Derivada de la funci√≥n log√≠stica')
    st.plotly_chart(logistic_der_fig)
    with st.expander('Plot Explanation'):
        st.write('Observe que la derivada de la funci√≥n log√≠stica se aproxima mucho a cero para entradas positivas y negativas grandes.')

    st.subheader('Pros')
    st.write('1. La funci√≥n log√≠stica introduce la no linealidad en la red, lo que le permite resolver problemas m√°s complejos que las funciones de activaci√≥n lineales.\n2. Es continua y diferenciable en todas partes.\n3. Como su salida est√° entre 0 y 1, es muy com√∫n utilizarla en la capa de salida en problemas de clasificaci√≥n binaria.')

    st.subheader('Contras')
    st.write("1. Sensibilidad limitada\n- La funci√≥n log√≠stica satura en la mayor parte de su dominio.\n- S√≥lo es sensible a las entradas alrededor de su punto medio 0,5.")
    st.write("2. Debido a que la funci√≥n log√≠stica puede saturarse f√°cilmente con entradas grandes, su gradiente se acerca mucho a cero. Esto hace que los gradientes sean cada vez m√°s peque√±os a medida que la retropropagaci√≥n avanza hacia las capas inferiores de la red.\n- Finalmente, los pesos de las capas inferiores reciben actualizaciones muy peque√±as y nunca convergen a sus valores √≥ptimos.")

    
## Funcion Tangente Hiperbolica
if activation_function == 'Funci√≥n tangente hiperb√≥lica (Tanh)':
    st.header('Funci√≥n tangente hiperb√≥lica (Tanh)')

    st.subheader('Descripci√≥n')
    st.write('La funci√≥n tanh tambi√©n tiene forma de "S" sigmoidea.')
    st.markdown(r'$tanh(z)=\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$')
    st.write('El rango de la funci√≥n tanh est√° comprendido entre -1 y 1.')

    st.subheader('Plot')
    tanh_fig = plot_function(np.tanh, title='Funci√≥n Hip√©rbolica Tangente')
    tanh_fig.add_annotation(x=7, y=1, text='<b>Saturation</b>', showarrow=True,
     font=dict(family="Montserrat", size=16, color="#1F8123"),
        align="center",arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor="#A835E1", ax=-20, ay=30,)
    tanh_fig.add_annotation(x=-7, y=-1, text='<b>Saturation</b>', showarrow=True,
     font=dict(family="Montserrat", size=16, color="#1F8123"),
        align="center",arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor="#A835E1", ax=0, ay=-30,)
    st.plotly_chart(tanh_fig)
    with st.expander('Plot Explanation'):
        st.write('- La funci√≥n tanh se satura a medida que las entradas son mayores (positivas o negativas).')
        st.write('- Para valores positivos y negativos grandes, la funci√≥n se aproxima asint√≥ticamente a 1 y -1, respectivamente.')
        st.write('- Cuando la funci√≥n se satura, su gradiente se aproxima mucho a cero, lo que ralentiza el aprendizaje.')
    
    st.subheader('Derivada')
    st.markdown(r'$tanh^{\prime}(z)= 1 - (tanh(z))^{2}$')
    st.text("")
    tanh_der_fig = plot_function_derivative(np.tanh, title='Derivada de la Funci√≥n Tanh')
    st.plotly_chart(tanh_der_fig)
    with st.expander('Plot Explanation'):
        st.write('Observe que la derivada de la funci√≥n tanh se aproxima mucho a cero para entradas positivas y negativas grandes.')
    
    st.subheader('Pros')
    st.write("1. La funci√≥n tanh introduce no linealidad en la red, lo que le permite resolver problemas m√°s complejos que las funciones de activaci√≥n lineales.\n2. Es continua, diferenciable y tiene derivadas distintas de cero en todas partes.\n3. Como su valor de salida oscila entre -1 y 1, hace que la salida de cada capa est√© m√°s o menos centrada en 0 al principio del entrenamiento, lo que acelera la convergencia.")

    st.subheader('Contras')
    st.write("1. Sensibilidad limitada\nLa funci√≥n tanh satura en la mayor parte de su dominio. S√≥lo es sensible a las entradas alrededor de su punto medio 0.")
    st.write("2. Gradientes de fuga en redes neuronales profundas - Debido a que la funci√≥n tanh puede saturarse f√°cilmente con entradas grandes, su gradiente se acerca mucho a cero. Esto hace que los gradientes se hagan cada vez m√°s peque√±os a medida que la retropropagaci√≥n avanza hacia las capas inferiores de la red. Finalmente, los pesos de las capas inferiores reciben actualizaciones muy peque√±as y nunca convergen a sus valores √≥ptimos..")

    st.markdown("**Nota**: el problema del gradiente de fuga es menos grave con la funci√≥n tanh porque tiene una media de 0 (en lugar de 0,5 como la funci√≥n log√≠stica).).")

    

## Funci√≥n unitaria lineal exponencial
if activation_function == 'Funci√≥n unitaria lineal exponencial':
    st.title('Funci√≥n unitaria lineal exponencial)')

    st.subheader('Descripci√≥n')
    st.markdown(r'$$ELU_{\alpha}(z)= \left\{\begin{array}{ll}z & z>0 \\{\alpha}(exp(z)-1) & z<=0 \\\end{array}\right.$$')

    st.write('La funci√≥n unitaria lineal exponencial mostrar√° la entrada directamente si es positiva (funci√≥n de identidad).')
    st.write('La salida de esta funci√≥n es negativa para entradas negativas dependiendo del valor de Œ±')

    st.subheader('Plot')

    with st.sidebar.form('leakage'):
        alpha = st.slider('Œ± Value', 0.0, 1.0, 0.2)
        st.form_submit_button('Aplicar cambios')

    def elu(z, alpha=alpha):
        return np.where(z < 0, alpha * (np.exp(z) - 1), z)

    elu_fig = plot_function(elu, title="Funci√≥n de unidad lineal exponencial (ELU)", alpha=alpha)
    st.plotly_chart(elu_fig)

    with st.expander('Plot Explanation'):
        st.write("- Este gr√°fico cambiar√° autom√°ticamente cuando cambie el valor de Œ± en el control deslizante de la barra lateral.")
        st.write('- la salida de la funci√≥n nunca es un cero verdadero para entradas negativas,')
        st.write('- El valor de Œ± suele fijarse en 1, o elegirse en el intervalo de [0,1 y 0,3].')
        st.write('- Si Œ± es igual a 1, la funci√≥n es suave en todas partes (optimizaci√≥n m√°s f√°cil).')
    
    st.subheader('Derivada')
    st.markdown(r'$$ELU^{\prime}(z)= \left\{\begin{array}{ll}1 & z>0 \\{\alpha}*exp(z) & z<=0 \\\end{array}\right.$$')

    elu_der_fig = plot_function_derivative(elu, title='Derivative of ELU')
    st.plotly_chart(elu_der_fig)

    with st.expander('Plot Explanation'):
        st.write("- Este gr√°fico cambiar√° autom√°ticamente cuando cambie el valor de Œ± en el control deslizante de la barra lateral.")
        st.write("- La funci√≥n tiene un gradiente distinto de cero para entradas negativas.")
    
    st.subheader('Pros')
    st.write("1. Aliviar el problema de la desaparici√≥n del gradiente")
    st.write("2. Evita el problema de los ReLUs muertos")
    st.write("3. Convergencia m√°s r√°pida\n- Cuando el valor de Œ± es igual a 1, la funci√≥n es suave en todas partes, lo que acelera el descenso del gradiente.")
    st.write("4. Mejor rendimiento.\n- La funci√≥n ELU supera a la mayor√≠a de las dem√°s variantes de ReLU con un tiempo de entrenamiento reducido.")

    st.subheader("Contras")
    st.write("1. Computacionalmente caro\n- Debido a que utiliza la funci√≥n exponencial, el ELU es m√°s lento de calcular que otras variantes de ReLU.")
    
## Derivada
if activation_function == 'Derivada Parcial':
    st.title('Derivada Parcial')
    st.write('la derivada parcial de una funci√≥n de varias variables es la derivada con respecto a cada una de esas variables manteniendo las otras como constantes.')
    st.markdown('La derivada parcial de una funci√≥n $f(x,y,\dots)$ con respecto a la variable $x$ se puede denotar de distintas maneras:')
    st.latex(r'''\frac{\partial f}{\partial x},\frac{\partial}{\partial x}f,D_1f,\partial_x f,f^\prime_x\text{ o } f_x.''')
    st.subheader('Definicion Formal')
    st.write('An√°logamente a la derivada ordinaria (funci√≥n de una variable real), la derivada parcial est√° definida como un l√≠mite.')
    st.markdown('Sea U es un subconjunto abierto de $\mathbb {R} ^{n}$ y $f:U\to \mathbb {R}$ una funci√≥n, la derivada parcial de f en el punto $\mathbf {a} =(a_{1},\dots ,a_{n})\in U$ con respecto a la i-√©sima variable $x_{i}$ se define como:')
    st.latex(r'''\frac{ \partial }{\partial x_i }f(\mathbf{a}) =
\lim_{h \rightarrow 0}{ 
f(a_1, \dots , a_{i-1}, a_i+h, a_{i+1}, \dots ,a_n) - 
f(a_1, \dots ,a_n) \over h }''')
    st.write('Si existe el l√≠mite')
    st.latex('La derivada parcial \frac {\partial f}{\partial x}')
    st.markdown('puede ser vista como otra funci√≥n definida sobre $U$ y puede ser de nuevo derivada de forma parcial. Si todas las derivadas parciales mixtas de segundo orden son continuas en un punto, entonces $f$ es una funci√≥n $C^{2}$ en ese punto; en tal caso, las derivadas parciales pueden ser intercambiadas por el teorema de Clairaut:')
    st.latex(r'''\frac{\partial^2f}{\partial x_i\partial x_j}=\frac{\partial^2f}{\partial x_j\partial x_i}''')
    st.subheader('Ejemplo')
    st.markdown('El volumen $V$ de un cono que depende de la altura del cono $h$ y su radio $r$, est√° dado por la f√≥rmula')
    st.latex(r'''V(r,h) = \frac{\pi r^2h}{3}''')
    st.markdown('Las derivadas parciales de $V$ respecto a $r$ y $h$ son')
    st.latex(r'''\begin{align}
    \frac{\partial V}{\partial r}&=\frac{2\pi rh}{3} \\
    \frac{ \partial V}{\partial h}&=\frac{\pi r^2}{3}
\end{align}''')
    st.write('respectivamente, la primera de ellas representa la tasa a la que el volumen del cono cambia si el radio var√≠a y su altura se mantiene constante, la segunda de ellas representa la tasa a la que el volumen cambia si la altura var√≠a y su radio se mantiene constante.')
    st.markdown('La derivada total de $V$ con respecto a $r$ y $h$ son')
    st.latex(r'''\frac{ d V}{d r}=\underbrace{\frac{2\pi rh}{3}}_{\frac{\partial V}{\partial r}}+\underbrace{\frac{\pi r^2}{3}}_{\frac{\partial V}{\partial h}}\frac{dh}{dr} ''')
    st.write('y')
    st.latex(r'''frac{d V}{d h}=\underbrace{\frac{\pi r^2}{3}}_{\frac{\partial V}{\partial h}}+\underbrace{\frac{2\pi rh}{3}}_{\frac{\partial V}{\partial r}}\frac{dr}{dh} ''')
    st.write('respectivamente.')
    
## Multiplicadores de Lagrange
if activation_function == 'Multiplicadores de Lagrange':
    st.title('Multiplicadores de Lagrange')
    st.markdown('Consideremos un caso tridimensional. Supongamos que tenemos la funci√≥n, $f(x, y)$, y queremos maximizarla, estando sujeta a la condici√≥n:')
    st.markdown('$g(x,y)=c$')
    st.markdown('donde $c$ es una constante. Podemos visualizar las curvas de nivel de $f$ dadas por')
    st.markdown('$f(x,y)=d_{n}$')
    st.markdown('para varios valores de $d_{n}$, y el contorno de g dado por $g(x, y) = c$. Supongamos que hablamos de la curva de nivel donde $g = c$. Entonces, en general, las curvas de nivel de $f$ y $g$ ser√°n distintas, y la curva $g = c$ por lo general intersectar√° y cruzar√° muchos contornos de $f$. En general, movi√©ndose a trav√©s de la l√≠nea $g=c$ podemos incrementar o disminuir el valor de $f$. S√≥lo cuando $g=c$ (el contorno que estamos siguiendo) toca tangencialmente (no corta) una curva de nivel de $f$, no se incrementa o disminuye el valor de $f$. Esto ocurre en el extremo local restringido y en los puntos de inflexi√≥n restringidos de $f$.')
    st.markdown('Geom√©tricamente traducimos la condici√≥n de tangencia diciendo que los gradientes de $f$ y $g$ son vectores paralelos en el m√°ximo. Introduciendo un nuevo escalar, $Œª$, resolvemos')
    st.latex(r'''\nabla</math>[''f''(''x'', ''y'') - Œª (''g''(''x'', ''y'') ‚àí ''c'')] = 0
para Œª ‚â† 0.''')
    st.markdown('Una vez determinados los valores de $Œª$, volvemos al n√∫mero original de variables y as√≠ continuamos encontrando el extremo de la nueva ecuaci√≥n no restringida')
    st.latex(r'''F(x,y)=f(x,y)-\lambda (g(x,y)-c)''')
    st.markdown('de forma tradicional. Eso es, $F(x,y)=f(x,y)$ para todo $(x, y)$ satisfaciendo la condici√≥n porque $g(x,y)-c$ es igual a cero en la restricci√≥n, pero los ceros de $‚àáF(x, y)$ est√°n todos en $g(x,y)=c$.')
    st.subheader('El m√©todo de los multiplicadores de Lagrange')
    st.markdown('Sea $f(x)$ una funci√≥n definida en un conjunto abierto $n-dimensional$ $\{x ‚àà \mathbb R ^{n}\}$ Se definen $s$ restricciones $g_{k}(x) = 0,k=1,..., s$, y se observa que:')
    st.latex(r'''h(\mathbf x, \mathbf \lambda) = f + \sum_{k=1}^s \lambda_k g_k ''')
    st.markdown('Se procede a buscar un extremo para $h$')
    st.latex(r'''\frac{\partial h}{\partial x_i} = 0,''')
    st.write('lo que es equivalente a')
    st.latex(r'''\frac{\partial f}{\partial x_i} = -\sum_k^s \lambda_k \frac{\partial g_k}{\partial x_i}.''')
    st.subheader('Ejemplo')
    st.write('Supongamos que queremos encontrar la distribuci√≥n probabil√≠stica discreta con m√°xima entrop√≠a. Entonces')
    st.latex(r'''f(p_1,p_2,\ldots,p_n) = -\sum_{k=1}^n p_k\log_2 p_k.''')
    st.latex(r'''g(p_1,p_2,\ldots,p_n)=\sum_{k=1}^n p_k=1.''')
    st.markdown('Podemos usar los multiplicadores de Lagrange para encontrar el punto de m√°xima entrop√≠a (dependiendo de las probabilidades). Para todo $k$ desde 1 hasta $n$, necesitamos')
    st.latex(r'''\frac{\partial}{\partial p_k}(f+\lambda (g-1))=0,''')
    st.write('lo que nos da')
    st.latex(r'''\frac{\partial}{\partial p_k}\left(-\sum_{k=1}^n p_k \log_2 p_k + \lambda\sum_{k=1}^n p_k - \lambda\right) = 0.''')
    st.markdown('Derivando estas $n$ ecuaciones obtenemos')
    st.latex(r'''-\left(\frac{1}{\ln 2}+\log_2 p_k \right) + \lambda = 0.''')
    st.markdown('Esto muestra que todo $p_{i}$ es igual (debido a que depende solamente de $Œª$). Usando la restricci√≥n $‚àë_{k} p_{k} = 1$ encontramos .')
    st.latex(r'''p_k = \frac{1}{n}.''')
    st.write('√âsta (la distribuci√≥n uniforme discreta) es la distribuci√≥n con la mayor entrop√≠a.')
    
## Matriz Jacobiana
if activation_function == 'Matriz Jacobiana':
    st.title('Matriz Jacobiana')
    st.write('La matriz jacobiana es una matriz formada por las derivadas parciales de primer orden de una funci√≥n. Una de las aplicaciones m√°s interesantes de esta matriz es la posibilidad de aproximar linealmente a la funci√≥n en un punto. En este sentido, el jacobiano representa la derivada de una funci√≥n multivariable. Propiamente deber√≠amos hablar m√°s que de matriz jacobiana, de diferencial jacobiana o aplicaci√≥n lineal jacobiana ya que la forma de la matriz depender√° de la base o coordenadas elegidas. Es decir, dadas dos bases diferentes la aplicaci√≥n lineal jacobiana tendr√° componentes diferentes a√∫n trat√°ndose del mismo objeto matem√°tico. La propiedad b√°sica de la "matriz" jacobiana es la siguiente,')
    st.markdown('dada una aplicaci√≥n cualquiera $\mathbf {F}:\mathbb{R} ^{n}\to \mathbb{R} ^{m}$ continua se dir√° que es diferenciable si existe una aplicaci√≥n lineal $Œª$ tal que:')
    st.latex(r'''\lim_{\|\mathbf{x}-\mathbf{y}\|\to 0}
\frac{ \| (\mathbf{F}(\mathbf{x}) - \mathbf{F}(\mathbf{y})) -
\boldsymbol\lambda(\mathbf{x}-\mathbf{y}) \|}{\|\mathbf{x}-\mathbf{y}\|} = 0''')
    st.subheader('Determinante Jacobiano')
    st.markdown('Si $m=n$ entonces $\mathbf F$ es una funci√≥n que va de $\mathbb {R} ^{n}$ a $\mathbb {R} ^{n}$')
    st.write('y en este caso la matriz jacobiana es una matriz cuadrada, por lo que podemos calcular su determinante, este es conocido como el determinante jacobiano.')
    st.markdown('El determinante jacobiano en un punto dado nos da informaci√≥n importante sobre el comportamiento de $\mathbf F$ cerca de ese punto. Una funci√≥n continuamente diferenciable $\mathbf F$ es invertible cerca del punto $p \in \mathbb R$ si el determinante jacobiano en $p$ es no nulo. Este es el teorema de la funci√≥n inversa.')
    st.subheader('Ejemplo')
    st.latex(r'''La matriz jacobiana de la funci√≥n \mathbf{F}:\mathbb{R}^3\to\mathbb{R}^3 dada por \mathbf{F}(x_1,x_2,x_3)= (x_1,5x_3,4x_2^2 - 2x_3) cuyas funciones componentes son''')
    st.latex(r'''\begin{align}
    f_1&=x_1 \\
    f_2&=5x_3 \\
    f_3&=4x_2^2-2x_3
\end{align} ''')
    st.write('es')
    st.latex(r'''\mathbf{J}_{\mathbf{F}}(x_1,x_2,x_3)=
\begin{bmatrix}
    \cfrac{\partial f_1}{\partial x_1} & \cfrac{\partial f_1}{\partial x_2} & \cfrac{\partial f_1}{\partial x_3} \\
    \cfrac{\partial f_2}{\partial x_1} & \cfrac{\partial f_2}{\partial x_2} & \cfrac{\partial f_2}{\partial x_3} \\
    \cfrac{\partial f_3}{\partial x_1} & \cfrac{\partial f_3}{\partial x_2} & \cfrac{\partial f_3}{\partial x_3}
\end{bmatrix}
=
\begin{bmatrix}
    1 & 0 & 0 \\ 
    0 & 0 & 5 \\ 
    0 & 8x_2 & -2
\end{bmatrix}''')
    
## Bibliografia
if activation_function == 'Bibliografia Sugerida':
    st.title('Bibliografia Sugerida') 
    st.write('Apostol, T.M., Calculus, Volumen I. M√©xico: Ed. Revert√©, 2001.')
    st.write('Courant, R., Differential and Integral Calculus, Volumen II. New York: J. Wiley, 1936.')
    st.write('Lang, S., Calculus of Several Variables. New York: Springer, 1987')
    st.write('Marsden, J., Tromba, A., C√°lculo Vectorial. M√©xico: Addison-Wesley, Pearson Educaci√≥n,1998.')
    st.write('Thomas, G.B., Finney, R.L., C√°lculo: varias variables. M√©xico: Adisson-Wesley Longman,1999')
    st.write('Budak, B.M., Fomin, S.V., Multiple Integrals Field Theory and Series. Mosc√∫: MIR,1973.')
    st.write('Spivak, M., C√°lculo en Variedades. M√©xico: Ed. Revert√©, 1987.')
    st.write('Spivak, M., C√°lculo Infinitesimal (2a ed.). M√©xico: Ed Revert√©, 1998.')